{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence model with Kafka and Spark streaming \n",
    "\n",
    "This notebook provides an example of a persistent model on streaming data coming from a Kafka producer. \n",
    "\n",
    "This notebook uses \n",
    "* the [Python client for the Apache Kafka distributed stream processing system](http://kafka-python.readthedocs.io/en/master/index.html) to receive messages from a Kafka cluster. \n",
    "* [Spark streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html) for processing the streaming data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re, ast\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[*] pyspark-shell'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"PersistenceReceive\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Connect to Kafka server on topic persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function creates a connection to a Kafka stream\n",
    "#You may change the topic, or batch interval\n",
    "#The Zookeeper server is assumed to be running at 127.0.0.1:2181\n",
    "#The function returns the Spark context, Spark streaming context, and DStream object\n",
    "def getKafkaDStream(spark,topic='persistence',batch_interval=10):\n",
    "\n",
    "    #Get Spark context\n",
    "    sc=spark.sparkContext\n",
    "\n",
    "    #Create streaming context, with required batch interval\n",
    "    ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "    #Checkpointing needed for stateful transforms\n",
    "    ssc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "    #Create a DStream that represents streaming data from Kafka, for the required topic \n",
    "    dstream = KafkaUtils.createStream(ssc, \"zoo1:2181,zoo2:2181,zoo3:2181\", \"persistence\", {topic: 1})\n",
    "    \n",
    "    return [sc,ssc,dstream]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictDay(data):\n",
    "    global state,day\n",
    "    new_values = data.collect()\n",
    "    if len(new_values) > 0:\n",
    "        sensorsToPredict = state[1]\n",
    "        array_values=np.array(data)\n",
    "        predictions = dict()\n",
    "        truths = dict()\n",
    "        seconds = dict()\n",
    "        last_temperature = dict()\n",
    "        for sensor in sensorsToPredict:\n",
    "            last_temperature[sensor] = state[0][sensor]\n",
    "            truths[sensor] = []\n",
    "            predictions[sensor] = []\n",
    "            seconds[sensor] = []\n",
    "        for i,array_ in new_values:\n",
    "            if i != array_[2]:\n",
    "                print(False)\n",
    "            if array_[2] in sensorToPredict:\n",
    "                s = array_[2]\n",
    "                truths[s].append(array_[0])\n",
    "                seconds[s].append(array_[1])\n",
    "                predictions[s].append(last_temperature[s])\n",
    "        \n",
    "        for sensor in sensorsToPredict:\n",
    "            last_temperature[s] = truths[sensor][-1]\n",
    "            MSE=np.mean((np.array(truths[sensor])-np.array(predictions[sensor]))**2)\n",
    "            print(day,sensor,MSE)\n",
    "\n",
    "            fig1,ax1 = plt.subplots()\n",
    "            ax1.scatter(seconds[sensor],predictions[sensor],label='prediction',marker='.', alpha=0.5)#, linestyle='None')\n",
    "            ax1.scatter(seconds[sensor],truths[sensor],label='truth',marker='.', alpha=0.5)#, linestyle='None')\n",
    "            plt.title('Temperature for sensor {0} on day {1}'.format(sensor,day))\n",
    "            plt.ylabel('Temperature in Â°C')\n",
    "            plt.xlabel('Time t in seconds')\n",
    "            plt.legend()\n",
    "#             plt.show()\n",
    "            plt.savefig('sensor_{0}_day_{1}.pdf'.format(sensor,day))\n",
    "        day+=1\n",
    "        state = [last_temperature,sensorsToPredict]\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define streaming pipeline\n",
    "\n",
    "* We define one state, which is a list of two elements:\n",
    "    * The last measurement\n",
    "    * The output of predictions for sensor 1 for day 8\n",
    "* We create a DStream, flat map with the sensor ID as key, update state for the stream, and save MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "#Print number of partitions and number of records for an RDD\n",
    "def printInfoRDD(rdd):\n",
    "    if rdd is not None:\n",
    "        print(\"The RDD has \"+str(rdd.getNumPartitions())+\" partitions\")\n",
    "        print(\"The RDD has \"+str(rdd.count())+\" elements\")\n",
    "    else:\n",
    "        print(\"No info to provide\")\n",
    "        \n",
    "#Save state in global Python variable\n",
    "def saveState(rdd):\n",
    "    global state_global\n",
    "    if rdd is not None:\n",
    "        print('save')\n",
    "        data=rdd.collect()\n",
    "#         print(data)\n",
    "        state_global=data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initial state\n",
    "last_measurement={1:0,24:0}\n",
    "sensorToPredict=[1,24]\n",
    "output_day8=None\n",
    "day = 1\n",
    "state=[last_measurement,sensorToPredict]\n",
    "\n",
    "#Batch interval (to be synchronized with KafkaSend)\n",
    "interval=10\n",
    "\n",
    "#This variable is used to retrieve state data (through saveState function)\n",
    "state_global=None\n",
    "\n",
    "#Create dtsream\n",
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='persistence',batch_interval=interval)\n",
    "\n",
    "#Evaluate string content (a list) and cast as float value\n",
    "\n",
    "dstream = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "#Use this for debugging\n",
    "\n",
    "#Group by sensor id. x[2] is here the sensorId (for example '1'), and x are the sensor measurement, seconds, sensorId and sensor type)\n",
    "dstream=dstream.flatMap(lambda x: [(x[2],x)])\n",
    "\n",
    "dstream.foreachRDD(predictDay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start streaming application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 404.496782137\n",
      "1 24 411.965460532\n",
      "2 1 469.207844804\n",
      "2 24 50.0523572009\n",
      "3 1 10.0465324233\n",
      "3 24 64.218765491\n",
      "4 1 8.75933262154\n",
      "4 24 89.2870464723\n",
      "5 1 10.6557572088\n",
      "5 24 75.6930710504\n",
      "6 1 6.90785660112\n",
      "6 24 25.0309479998\n",
      "7 1 14.2250965906\n",
      "7 24 19.1663016444\n",
      "8 1 14.6868126419\n",
      "8 24 27.6863858409\n"
     ]
    }
   ],
   "source": [
    "#For synchronization with receiver (for the sake of the simulation), starts at a number of seconds multiple of five\n",
    "current_time=time.time()\n",
    "time_to_wait=interval-current_time%interval\n",
    "time.sleep(time_to_wait)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
