{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Least Square with Kafka and Spark streaming \n",
    "\n",
    "This notebook provides an example for estimating the coefficients of a linear model on streaming data coming from a Kafka producer. The coefficient estimation is achieved using the recursive least square (RLS) algorithm, using two different RLS models in parallel (with different forgetting factors).\n",
    "\n",
    "The linear model has 10 parameters, with coefficients [1,0,0,0,0,0,0,0,0,1] (see notebook KafkaSendRLS).\n",
    "\n",
    "This notebook uses \n",
    "* the [Python client for the Apache Kafka distributed stream processing system](http://kafka-python.readthedocs.io/en/master/index.html) to receive messages from a Kafka cluster. \n",
    "* [Spark streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html) for processing the streaming data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re, ast\n",
    "import numpy as np\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[*] pyspark-shell'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"KafkaReceive\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Connect to Kafka cluster on topic dataLinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function creates a connection to a Kafka stream\n",
    "#You may change the topic, or batch interval\n",
    "#The Zookeeper server is assumed to be running at 127.0.0.1:2181\n",
    "#The function returns the Spark context, Spark streaming context, and DStream object\n",
    "def getKafkaDStream(spark,topic='persistence',batch_interval=10):\n",
    "\n",
    "    #Get Spark context\n",
    "    sc=spark.sparkContext\n",
    "\n",
    "    #Create streaming context, with required batch interval\n",
    "    ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "    #Checkpointing needed for stateful transforms\n",
    "    ssc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "    #Create a DStream that represents streaming data from Kafka, for the required topic \n",
    "    dstream = KafkaUtils.createStream(ssc, \"zoo1:2181,zoo2:2181,zoo3:2181\", \"spark-streaming-consumer\", {topic: 1})\n",
    "    \n",
    "    return [sc,ssc,dstream]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    ## RLS update function\n",
    "    ## Only update with first value of RDD. You should transofrm new_values to array, and update models for all values \n",
    "    if (len(new_values)>0 ):\n",
    "        \n",
    "        key=new_values[0][0]\n",
    "        yx=new_values[0][1]\n",
    "        i=yx[0]\n",
    "        y=yx[1]\n",
    "        x=yx[2:]\n",
    "        n=len(x)\n",
    "        \n",
    "        beta=state[1]\n",
    "        beta.shape=(n,1)\n",
    "        V=state[2]\n",
    "        mu=state[3]\n",
    "        sse=state[4]  ## sum of squared errors\n",
    "        N=state[5]   ## number of treated samples\n",
    "        x.shape=(1,n)\n",
    "        err=y-x.dot(beta)\n",
    "        sse=sse+pow(err,2.0)\n",
    "        V=1.0/mu*(V-V.dot(x.T).dot(x).dot(V)/(1.0+float(x.dot(V).dot(x.T))))\n",
    "        gamma=V.dot(x.T)\n",
    "        beta=beta+gamma*err\n",
    "        \n",
    "        return (key,beta,V,mu,sse/(N+1.0),N+1)  ## update formula mod1\n",
    "        \n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define streaming pipeline\n",
    "\n",
    "* We define a stream with two states, for updating two RLS models in paralell. Each state contains a state of variables to keep the state of the model, as well as to keep track of MSE estimates. A state is a list of 5 elements:\n",
    "    * The first three are beta, V and mu, and define the state of the model (see RLS formulas in course)\n",
    "    * The last two are an estimate of the MSE of the model, and the number of treated samples \n",
    "* We create a DStream, flat map with the sensor ID as key, update state for the stream, and save MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, ast\n",
    "\n",
    "n=10 # number of features\n",
    "\n",
    "beta1=np.zeros(n)  ## initial parameter vector for model 1\n",
    "V1=np.diag(np.zeros(n)+10) ## initial covariance matrix for model 1\n",
    "mu1=1.0 # forgetting factor for model 1\n",
    "\n",
    "state1=('mod1',beta1,V1,mu1,0,0)\n",
    "\n",
    "beta2=np.zeros(n)  ## initial parameter vector for model 2\n",
    "V2=np.diag(np.zeros(n)+1) ## initial covariance matrix for model 2\n",
    "mu2=0.99 # forgetting factor for model 2\n",
    "\n",
    "state2=('mod2',beta2,V2,mu2,0,0)\n",
    "\n",
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='dataLinearModel',batch_interval=1)\n",
    "\n",
    "#Evaluate input (a list - see KafkaSendRLS notebook)\n",
    "dstream = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "#dstream.pprint()\n",
    "\n",
    "dstream=dstream.flatMap(lambda x: [('mod1',('mod1',1.0*np.array(x))),\n",
    "                            ('mod2',('mod2',1.0*np.array(x)))])\n",
    "#dstream.pprint()\n",
    "\n",
    "initialStateRDD = sc.parallelize([(u'mod1', state1),\n",
    "                                  (u'mod2', state2)])\n",
    "\n",
    "dstream=dstream.updateStateByKey(updateFunction,initialRDD=initialStateRDD)\n",
    "\n",
    "#Only display beta and error\n",
    "#beta should converge to [1,0,0,0,0,0,0,0,0,1] (send KafkaSend notebook)\n",
    "dstream.map(lambda x: x[1][0:2]+x[1][4:6]).pprint()\n",
    "#dstream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start streaming application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
