{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re, ast\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[*] pyspark-shell'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"KafkaReceive\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "Name: ClusterId, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "locs = pd.read_csv(\"../../data/mote_locs.txt\", header=None, sep=\" \")\n",
    "locs.columns = [\"SensorId\", \"X\", \"Y\"]\n",
    "ids = locs.values[:, 0]\n",
    "pos = locs.values[:, 1:]\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(pos)\n",
    "labels = np.vstack((ids, kmeans.labels_)).T.astype(int)\n",
    "clusters = pd.DataFrame({'SensorId': labels[:,0], 'ClusterId': labels[:,1]})\n",
    "\n",
    "clusters.loc[clusters.SensorId==1].ClusterId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClusterId</th>\n",
       "      <th>SensorId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ClusterId  SensorId\n",
       "0           2         1\n",
       "1           2         2\n",
       "2           2         3\n",
       "32          2        33\n",
       "33          2        34\n",
       "34          2        35\n",
       "35          2        36\n",
       "36          2        37\n",
       "37          2        38\n",
       "38          2        39\n",
       "39          2        40\n",
       "40          2        41\n",
       "41          2        42\n",
       "42          2        43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sensor to predict\n",
    "\n",
    "target_sensor_id = 1\n",
    "target_cluster_id = int(clusters.loc[clusters.SensorId==target_sensor_id].ClusterId)\n",
    "clusters.loc[clusters.ClusterId == target_cluster_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function creates a connection to a Kafka stream\n",
    "#You may change the topic, or batch interval\n",
    "#The Zookeeper server is assumed to be running at 127.0.0.1:2181\n",
    "#The function returns the Spark context, Spark streaming context, and DStream object\n",
    "def getKafkaDStream(spark,topic='persistence',batch_interval=10):\n",
    "\n",
    "    #Get Spark context\n",
    "    sc=spark.sparkContext\n",
    "\n",
    "    #Create streaming context, with required batch interval\n",
    "    ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "    #Checkpointing needed for stateful transforms\n",
    "    ssc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "    #Create a DStream that represents streaming data from Kafka, for the required topic \n",
    "    dstream = KafkaUtils.createStream(ssc, \"zoo1:2181,zoo2:2181,zoo3:2181\", \"spark-streaming-consumer\", {topic: 1})\n",
    "    \n",
    "    return [sc,ssc,dstream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save state in global Python variable\n",
    "def saveState(rdd):\n",
    "    global state_global\n",
    "    if rdd is not None:\n",
    "        data=rdd.collect()\n",
    "        state_global=data\n",
    "        \n",
    "def printInfoRDD(rdd):\n",
    "    clear_output(wait=True)\n",
    "    if rdd is not None:\n",
    "        print(\"The RDD has \"+str(rdd.getNumPartitions())+\" partitions\")\n",
    "        print(\"The RDD has \"+str(rdd.count())+\" elements\")\n",
    "    else:\n",
    "        print(\"No info to provide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateFunction(new_values, state): \n",
    "    ## RLS update function\n",
    "    ## Only update with first value of RDD. You should transform new_values to array, and update models for all values \n",
    "    if (len(new_values)>0 ):\n",
    "        \n",
    "        key=new_values[0][0]\n",
    "        yx=new_values[0][1]\n",
    "        i=yx[0]\n",
    "        y=yx[1]\n",
    "        x=yx[2:]\n",
    "        n=len(x)\n",
    "        \n",
    "        beta=state[1]\n",
    "        beta.shape=(n,1)\n",
    "        V=state[2]\n",
    "        mu=state[3]\n",
    "        sse=state[4]  ## sum of squared errors\n",
    "        N=state[5]   ## number of treated samples\n",
    "        x.shape=(1,n)\n",
    "        err=y-x.dot(beta)\n",
    "        sse=sse+pow(err,2.0)\n",
    "        V=1.0/mu*(V-V.dot(x.T).dot(x).dot(V)/(1.0+float(x.dot(V).dot(x.T))))\n",
    "        gamma=V.dot(x.T)\n",
    "        beta=beta+gamma*err\n",
    "        \n",
    "        return (key,beta,V,mu,sse/(N+1.0),N+1)  ## update formula mod1\n",
    "        \n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateEMA(new_value, state):\n",
    "    # new_value = (x, y_true) where x in avg temp in cluster\n",
    "    # state = (alpha, EMA, SSE, N)\n",
    "    if len(new_value) > 0:\n",
    "        x = new_value[0][0]\n",
    "        y_true = new_value[0][1]\n",
    "        bin = new_value[0][2]\n",
    "        alpha = state[0]\n",
    "        EMA = state[1]\n",
    "        EMA = alpha*x + (1-alpha)*EMA\n",
    "        N = state[3]\n",
    "        SSE = state[2] * N\n",
    "        if y_true != None:\n",
    "            err = y_true - EMA      \n",
    "            SSE = SSE + pow(err,2)\n",
    "            N += 1\n",
    "        return (alpha, EMA, SSE/N, N, err**2, y_true, bin)\n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost:4242\n"
     ]
    }
   ],
   "source": [
    "OPENTSDB_URL = \"localhost:4242\" #'http://' + os.environ.get('OPENTSDB_URL')\n",
    "print(OPENTSDB_URL)\n",
    "\n",
    "def to_json(state):\n",
    "    y_true = dict()\n",
    "    y_pred = dict()\n",
    "    error = dict()\n",
    "    state = state[1]\n",
    "    y_true['metric'] = 'temperature.truth'\n",
    "    y_true['timestamp'] = state[-1]\n",
    "    y_true['value'] = state[5]\n",
    "    y_true['tags']['space'] = 1\n",
    "    \n",
    "    y_pred['metric'] = 'temperature.prediction'\n",
    "    y_pred['timestamp'] = state[-1]\n",
    "    y_pred['value'] = state[1]\n",
    "    y_pred['tags']['space'] = 1\n",
    "    \n",
    "    error['metric'] = 'temperature.error'\n",
    "    error['timestamp'] = state[-1]\n",
    "    error['value'] = state[4]\n",
    "    error['tags']['space'] = 1\n",
    "    \n",
    "    data = [y_true, y_pred, error]\n",
    "    return data\n",
    "\n",
    "\n",
    "def sendPartition(iter):\n",
    "    if iter:\n",
    "        r = requests.post(OPENTSDB_URL + '/api/put', data=json.dumps(iter))\n",
    "        print(r.status_code)\n",
    "        return r.status_code\n",
    "    else:\n",
    "        r = 400\n",
    "        print(r)\n",
    "        return r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, ast\n",
    "\n",
    "time_resolution = 10\n",
    "wait_time = 1\n",
    "simulation = 1\n",
    "\n",
    "alpha1 = 0.25\n",
    "state = (alpha1,20,0,0,0,20,0)\n",
    "\n",
    "# dstream value format: (bin, y_true, Value, seconds, SensorId, Type):\n",
    "if simulation:\n",
    "    [sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='ClusterRLSTrain',batch_interval=wait_time)\n",
    "    dstream = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "else:\n",
    "    [sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='ClusterRLSTrain',batch_interval=time_resolution)\n",
    "    dstream = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\\\n",
    "                     .window(time_resolution, time_resolution)\n",
    "\n",
    "# dstream value format: (ClusterId, (bin, y_true, Value, seconds, SensorId, Type)):\n",
    "dstream = dstream.map(lambda x: (int(clusters.loc[clusters.SensorId==int(x[4])].ClusterId), x))\n",
    "# keep only data belonging to target_cluster_id:\n",
    "dstream = dstream.filter(lambda x: x[0] == target_cluster_id)\n",
    "dstream = dstream.mapValues(lambda x: (x[2], 1, x[1], x[0])) # (x, 1, y_true, bin)\n",
    "# compute average of x:\n",
    "dstream = dstream.reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1], x[2], x[3]))\n",
    "dstream = dstream.mapValues(lambda x: (x[0]/x[1], x[2], x[3]))\n",
    "\n",
    "initialStateRDD = sc.parallelize([(target_cluster_id, state)])\n",
    "dstream = dstream.updateStateByKey(updateEMA,initialRDD=initialStateRDD)\n",
    "#dstream.pprint()\n",
    "stateStream = dstream.flatMap(lambda x: [{'metric': 'temperature.prediction',\n",
    "                                           'timestamp': time.time() + x[1][-1],\n",
    "                                           'value': x[1][1],\n",
    "                                           'tags': {'space': 1}},\n",
    "                                           {'metric': 'temperature.truth',\n",
    "                                           'timestamp': time.time() + x[1][-1],\n",
    "                                           'value': x[1][5],\n",
    "                                           'tags': {'space': 1}},\n",
    "                                           {'metric': 'temperature.error',\n",
    "                                           'timestamp': time.time() + x[1][-1],\n",
    "                                           'value': x[1][4],\n",
    "                                           'tags': {'space': 1}}\n",
    "                                          ])\n",
    "#stateStream.pprint()\n",
    "stateStream.foreachRDD(printInfoRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-05-29 15:51:01\n",
      "-------------------------------------------\n",
      "{'metric': 'temperature.prediction', 'timestamp': 1559145061.8363674, 'value': 20, 'tags': {'space': 1}}\n",
      "{'metric': 'temperature.truth', 'timestamp': 1559145061.8363698, 'value': 20, 'tags': {'space': 1}}\n",
      "{'metric': 'temperature.error', 'timestamp': 1559145061.8363702, 'value': 0, 'tags': {'space': 1}}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-29 15:51:02\n",
      "-------------------------------------------\n",
      "{'metric': 'temperature.prediction', 'timestamp': 1559145062.7725465, 'value': 20, 'tags': {'space': 1}}\n",
      "{'metric': 'temperature.truth', 'timestamp': 1559145062.772551, 'value': 20, 'tags': {'space': 1}}\n",
      "{'metric': 'temperature.error', 'timestamp': 1559145062.7725523, 'value': 0, 'tags': {'space': 1}}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-29 15:51:03\n",
      "-------------------------------------------\n",
      "{'metric': 'temperature.prediction', 'timestamp': 1559145063.7765334, 'value': 20, 'tags': {'space': 1}}\n",
      "{'metric': 'temperature.truth', 'timestamp': 1559145063.7765377, 'value': 20, 'tags': {'space': 1}}\n",
      "{'metric': 'temperature.error', 'timestamp': 1559145063.7765386, 'value': 0, 'tags': {'space': 1}}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-29 15:51:04\n",
      "-------------------------------------------\n",
      "{'metric': 'temperature.prediction', 'timestamp': 1559145064.7525103, 'value': 20, 'tags': {'space': 1}}\n",
      "{'metric': 'temperature.truth', 'timestamp': 1559145064.7525141, 'value': 20, 'tags': {'space': 1}}\n",
      "{'metric': 'temperature.error', 'timestamp': 1559145064.752515, 'value': 0, 'tags': {'space': 1}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-05-29 15:51:05\n",
      "-------------------------------------------\n",
      "{'metric': 'temperature.prediction', 'timestamp': 1559145065.732522, 'value': 20, 'tags': {'space': 1}}\n",
      "{'metric': 'temperature.truth', 'timestamp': 1559145065.732526, 'value': 20, 'tags': {'space': 1}}\n",
      "{'metric': 'temperature.error', 'timestamp': 1559145065.7325268, 'value': 0, 'tags': {'space': 1}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
